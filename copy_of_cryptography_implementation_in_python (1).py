# -*- coding: utf-8 -*-
"""Copy of Cryptography_Implementation_in_Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q45WXO0kdpkFbJaRdVveos6DHtI3LWYw


# Importing Important Libraries
Importing tensorflow
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# %matplotlib inline
import matplotlib.pyplot as plt

# Importing Pandas as pd
import pandas as pd

# Importing numpy as np
import numpy as np

"""# Initalizing Variables"""

BATCH_SIZE = 4096
MSG_SIZE = 16
KEY_SIZE = 16
N = MSG_SIZE + KEY_SIZE

"""# Generating a random message and corresponding key"""

key = 2 * tf.random.uniform([BATCH_SIZE, KEY_SIZE], minval=0, maxval=2, dtype=tf.int32) - 1 #Generates a random shared key for Alice and Bob, converted to float
key = tf.cast(key, tf.float32) #converts our key to float numbers

print(key)

alice_input = 2 * tf.random.uniform([BATCH_SIZE, MSG_SIZE], minval=0, maxval=2, dtype=tf.int32) - 1
#Generates a random plaintext message for Alice, also converted to float.
alice_input = tf.cast(key, tf.float32)
#Combines the message and key for encryption along the first dimension.
print(alice_input)

encrypt_msg = tf.concat([alice_input, key], 1)
print(encrypt_msg)

"""# Separation of Bob's Output and Original Message"""

def separation(alice_input, bob_output):
    #Distance from Alice where the message is sent upto the point of receiving end
    print("separation", tf.reduce_sum(tf.abs(tf.subtract(alice_input, bob_output)),reduction_indices=1))
    return tf.reduce_sum(tf.abs(tf.subtract(alice_input, bob_output)),reduction_indices=1)

"""# Eve's Guess v/s a Random Guess"""

def eve_vs_random_guess(alice_input, eve_output):
    print( "eve vs random guess", tf.square(MSG_SIZE/2 - separation(alice_input, eve_output)) / (MSG_SIZE/2) ** 2)
    return tf.square(MSG_SIZE/2 - separation(alice_input, eve_output)) / (MSG_SIZE/2) ** 2

"""# Minimizing Bob - Loss and Maximizing Eve - Loss"""

def combined_loss_function(alice_input, decipher_bob, eve_output):
  print("combined loss function " , separation(alice_input, decipher_bob)/MSG_SIZE + eve_vs_random_guess(alice_input, eve_output))
  return separation(alice_input, decipher_bob)/MSG_SIZE + eve_vs_random_guess(alice_input, eve_output)

"""# Eve's Loss"""

def attacker_loss(alice_input, eve_output):
  print("atacker loss" , separation(alice_input, eve_output) )
  return separation(alice_input, eve_output)

def weights(shape, name):
  initial = tf.truncated_normal(shape, stddev=1)
  return tf.Variable(initial, name=name)

"""# Building a common neural network for bob, eve and alice"""

def neural_network(encrypt_msg, name, initial_size):
  print(initial_size,N)

  net = tf.matmul(encrypt_msg, weights([initial_size, N], name="{}/fc_layer".format(name)))
  print("net" , net)

  net = tf.reshape(net, [BATCH_SIZE, N, 1])
  print("net" , net)

  net = tf.sigmoid(tf.nn.conv1d(net, weights([4, 1, 2], name="{}/conv1d-1".format(name)), stride=1, padding="SAME"))
  print("net" , net)

  net = tf.sigmoid(tf.nn.conv1d(net, weights([2, 2, 4], name="{}/conv1d-2".format(name)), stride=2, padding="SAME"))
  print("net" , net)

  net = tf.sigmoid(tf.nn.conv1d(net, weights([1, 4, 4], name="{}/conv1d-3".format(name)), stride=1, padding="SAME"))
  print("net" , net)

  net = tf.tanh(tf.nn.conv1d(net, weights([1, 4, 1], name="{}/conv1d-4".format(name)), stride=1, padding="SAME"))
  print("net" , net)

  net = tf.reshape(net, [BATCH_SIZE, MSG_SIZE])
  print("net" , net)

  return net

"""# Implementing Functions"""

alice_bob_cycles = 1
eve_cycles = 2

table = []
if __name__ == '__main__':
    sess=tf.compat.v1.InteractiveSession() #done so that somefunctions which run on older versions of tensorflow can run

    #Placeholder for the encoded message received by Bob and Eve (float, shape: batch size x message size).
    cipher_text = tf.placeholder(tf.float32, shape=[BATCH_SIZE, MSG_SIZE])
    print("initial cipher_text",cipher_text)

    #Placeholder for Bob's decoded message (float, same shape as cipher_text).
    decipher_bob = tf.placeholder(tf.float32, shape=[BATCH_SIZE, MSG_SIZE])



    #Placeholder for Eve's decoded message (float, same shape as cipher_text).
    eve_output = tf.placeholder(tf.float32, shape=[BATCH_SIZE, MSG_SIZE])



    #Alice's network processes the encrypted message through its architecture "cipher/a" with initial size N.
    cipher_text = neural_network(encrypt_msg, name="cipher/a", initial_size=N)
    print("cipher_text after neural network", cipher_text)

    #Bob's network combines the received cipher text with the shared key and processes it through his network "cipher/b" (same architecture as Alice's) to recover the original message.
    decipher_bob = neural_network(tf.concat([cipher_text, key], 1), name="cipher/b", initial_size=N)
    print("decipher_bob",decipher_bob)
    #Eve's network directly processes the received cipher text through her smaller network "attacker" to attempt eavesdropping.
    eve_output = neural_network(cipher_text, name="attacker", initial_size=MSG_SIZE)
    print("eve output" , eve_output)

    #Creates an Adam optimizer for adjusting network weights with a learning rate of 0.0008.
    adam = tf.train.AdamOptimizer(0.0008)


    #Collects all trainable variables within Alice & Bob's network ("cipher/").
    cipher_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "cipher/")
    print("cipher vars", cipher_vars)

    #Collects all trainable variables within Eve's network ("attacker").
    attacker_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "attacker/")
    print("attacker vars", attacker_vars)


    #Defines a training step for Alice & Bob, minimizing the combined_loss_function function on the optimizer while updating their variables.
    cipher_training_step = adam.minimize(combined_loss_function(alice_input, decipher_bob, eve_output), var_list=cipher_vars)

    #Defines a training step for Eve, minimizing the attacker_loss function on the optimizer while updating her variables.
    attacker_training_step = adam.minimize(attacker_loss(alice_input, eve_output), var_list=attacker_vars)

    #Measures the average bob_outputance between Alice's original message and Bob's decoded message, reflecting communication success.
    cipher_accuracy = tf.reduce_mean(separation(alice_input, decipher_bob))

    #Measures the average bob_outputance between Eve's decoded message and the original message, reflecting eavesdropping success.
    attacker_accuracy = tf.reduce_mean(attacker_loss(alice_input, eve_output))

    #Initializes all network variables before training.
    sess.run(tf.initialize_all_variables())
    for q in range(2000):

      #Every 100 iterations, it prints "step {}, training accuracy {}".
      #This prints the current iteration and a tuple containing the cipher_accuracy and attacker_accuracy values, showing how the training progresses for both sides.
      if q % 50 == 0:
        accuracy_of_training = cipher_accuracy.eval(), attacker_accuracy.eval()
        table.append([q,cipher_accuracy.eval(),attacker_accuracy.eval()])

       #print("step number : ",q,"Loss of Bob : ", cipher_accuracy.eval(),"Eve's Loss : ",attacker_accuracy.eval())


      #Runs the training step for Alice & Bob to update their network weights.
      for iteration in range(alice_bob_cycles):
        cipher_training_step.run()

      #Runs the training step for eve to update their network weights however to give eve some advantage it is run for 2 cycles
      for iteration in range(eve_cycles):
        attacker_training_step.run()



df = pd.DataFrame(table, columns=['Step Number', 'Bob Loss', 'Eve Loss'])
print("Bob Loss Graph")
plt.plot(df["Step Number"], df["Bob Loss"], label="Bob Loss")
plt.xlabel('Step Number')
plt.ylabel('Loss')
plt.title('Bob Loss Graph')
plt.legend()  # Add this line to display the legend

# Show the plot
plt.show()

# Print Eve Loss Graph
plt.plot(df["Step Number"], df["Eve Loss"], label="Eve Loss")
plt.xlabel('Step Number')
plt.ylabel('Loss')
plt.title('Eve Loss Graph')
plt.legend()  # Add this line to display the legend

# Show the plot
plt.show()

plt.show()
print(df)

